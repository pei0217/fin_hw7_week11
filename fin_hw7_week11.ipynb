{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSv7TyC2QnJ+zMFSJFAmJV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pei0217/fin_hw7_week11/blob/main/fin_hw7_week11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPDbMnI3gPg5",
        "outputId": "adbacf68-7df2-4538-f381-8298d97ed84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: 0.6779\n",
            "Epoch 2/15, Loss: 0.7108\n",
            "Epoch 3/15, Loss: 0.6710\n",
            "Epoch 4/15, Loss: 0.7192\n",
            "Epoch 5/15, Loss: 0.6923\n",
            "Epoch 6/15, Loss: 0.6950\n",
            "Epoch 7/15, Loss: 0.7081\n",
            "Epoch 8/15, Loss: 0.6816\n",
            "Epoch 9/15, Loss: 0.6864\n",
            "Epoch 10/15, Loss: 0.6930\n",
            "Epoch 11/15, Loss: 0.6890\n",
            "Epoch 12/15, Loss: 0.6976\n",
            "Epoch 13/15, Loss: 0.6345\n",
            "Epoch 14/15, Loss: 0.7170\n",
            "Epoch 15/15, Loss: 0.6509\n",
            "Accuracy: 60.05%\n"
          ]
        }
      ],
      "source": [
        "# 匯入必要套件\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# 下載 VADER Lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# ======= 數據處理 =======\n",
        "\n",
        "# 1. 加載數據\n",
        "news_data = pd.read_csv('Combined_News_DJIA(train).csv')\n",
        "stock_data = pd.read_csv('DJIA_table(train).csv')\n",
        "\n",
        "# 2. 處理日期格式\n",
        "news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
        "stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%d-%m-%Y', errors='coerce')\n",
        "\n",
        "# 3. 合併數據\n",
        "merged_data = pd.merge(stock_data, news_data, on='Date')\n",
        "\n",
        "# 4. 提取新聞情緒\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "merged_data['Sentiment'] = merged_data[['Top1', 'Top2', 'Top3']].fillna('').apply(\n",
        "    lambda row: sum(sia.polarity_scores(row[col])['compound'] for col in ['Top1', 'Top2', 'Top3']) / 3, axis=1\n",
        ")\n",
        "\n",
        "# 5. 增加技術指標特徵\n",
        "merged_data['Moving_Avg_5'] = merged_data['Close'].rolling(window=5).mean()\n",
        "merged_data['Volatility'] = merged_data['Close'].rolling(window=5).std()\n",
        "merged_data.fillna(0, inplace=True)  # 填充缺失值\n",
        "\n",
        "# 特徵選擇\n",
        "features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Sentiment', 'Moving_Avg_5', 'Volatility']\n",
        "scaler = MinMaxScaler()\n",
        "merged_data[features] = scaler.fit_transform(merged_data[features])\n",
        "\n",
        "# 6. 標籤\n",
        "labels = merged_data['Label']\n",
        "\n",
        "# 7. 分割數據集\n",
        "X = merged_data[features].values\n",
        "y = labels.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ======= 模型構建 =======\n",
        "\n",
        "# 8. 構建數據集\n",
        "train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# 9. 定義改進的雙向 LSTM 模型\n",
        "class BiLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.5):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                            batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # 雙向 LSTM 輸出加倍\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(num_layers * 2, x.size(0), hidden_size).to(x.device)  # 雙向 LSTM\n",
        "        c_0 = torch.zeros(num_layers * 2, x.size(0), hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x.unsqueeze(1), (h_0, c_0))  # 添加時間維度\n",
        "        out = self.dropout(out[:, -1, :])  # Dropout\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# 模型參數\n",
        "input_size = len(features)\n",
        "hidden_size = 128  # 增加隱藏層神經元數\n",
        "num_layers = 3     # 增加 LSTM 層數\n",
        "num_classes = 2\n",
        "dropout_rate = 0.5  # 增加 Dropout\n",
        "learning_rate = 0.0005\n",
        "model = BiLSTMModel(input_size, hidden_size, num_layers, num_classes, dropout_rate)\n",
        "\n",
        "# 10. 定義損失函數和優化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# ======= 訓練模型 =======\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "epochs = 15\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ======= 評估模型 =======\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO1w36rolqhL",
        "outputId": "79c531a9-aeca-451d-b223-f83a98fe385b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}